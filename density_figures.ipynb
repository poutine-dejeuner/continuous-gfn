{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env import Box, get_last_states\n",
    "import torch \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import sample_from_reward, get_test_states, fit_kde\n",
    "from torch.distributions import Distribution\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import os\n",
    "from model import CirclePF, CirclePB\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Box(dim=2, delta=0.25, epsilon=1e-10, verify_actions=False)\n",
    "samples = sample_from_reward(env, 20000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_states = torch.clone(samples)\n",
    "while True:\n",
    "    A = torch.where(\n",
    "            current_states[:, 0] >= env.delta,\n",
    "            0.0,\n",
    "            2.0 / torch.pi * torch.arccos((current_states[:, 0]) / env.delta),\n",
    "        )\n",
    "    B = torch.where(\n",
    "        current_states[:, 1] >= env.delta,\n",
    "        1.0,\n",
    "        2.0 / torch.pi * torch.arcsin((current_states[:, 1]) / env.delta),\n",
    "    )\n",
    "    theta = torch.rand(current_states.shape[0]) * (B - A) + A\n",
    "    to_change_mask = current_states.norm(dim=1) > env.delta\n",
    "    if to_change_mask.sum() == 0:\n",
    "        break\n",
    "    current_states[to_change_mask] =current_states[to_change_mask] - env.delta * torch.stack([torch.cos(np.pi / 2 * theta[to_change_mask]), torch.sin(np.pi / 2 * theta[to_change_mask])], dim=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde = KernelDensity(kernel=\"gaussian\",\n",
    "     bandwidth=0.01).fit(current_states.numpy())\n",
    "test_states, n_2 = get_test_states(maxi=env.delta, n=200)\n",
    "log_dens_uniform_pb = kde.score_samples(test_states)\n",
    "log_dens_uniform_pb[np.linalg.norm(test_states, axis=1) > env.delta] = np.nan\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open saved model \n",
    "relative_path = \"d0.25_tb_PBlearnable_lr0.001_lrZ0.001_sd111_n2_n04_eps0.0_min0.0_max1.0_shift0.0\"\n",
    "# relative_path = \"d0.25_tb_PBtied_lr0.001_lrZ0.05_sd12_n1_n04_eps0.0_min0.0_max1.0\"\n",
    "path = os.path.join(\"saved_models\", relative_path)\n",
    "# open args.json within relative path\n",
    "with open(os.path.join(path, \"args.json\"), \"r\") as f:\n",
    "    args = json.load(f)\n",
    "hidden_dim = args[\"hidden_dim\"]\n",
    "n_hidden = args[\"n_hidden\"]\n",
    "n_components = args[\"n_components\"]\n",
    "n_components_s0 = args[\"n_components_s0\"]\n",
    "beta_min = args[\"beta_min\"]\n",
    "beta_max = args[\"beta_max\"]\n",
    "model = CirclePF(\n",
    "    hidden_dim=hidden_dim,\n",
    "    n_hidden=n_hidden,\n",
    "    n_components=n_components,\n",
    "    n_components_s0=n_components_s0,\n",
    "    beta_min=beta_min,\n",
    "    beta_max=beta_max,\n",
    ")\n",
    "model.load_state_dict(torch.load(os.path.join(path, \"model.pt\")))\n",
    "bw_model = CirclePB(\n",
    "    hidden_dim=hidden_dim,\n",
    "    n_hidden=n_hidden,\n",
    "    n_components=n_components,\n",
    "    beta_min=beta_min,\n",
    "    beta_max=beta_max,\n",
    ")\n",
    "bw_model.load_state_dict(torch.load(os.path.join(path, \"bw_model.pt\")))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path_2 = \"d0.25_tb_PBlearnable_lr0.001_lrZ0.001_sd112_n2_n04_eps0.0_min0.0_max1.0_shift0.0\"\n",
    "relative_path_3 = \"d0.25_tb_PBlearnable_lr0.001_lrZ0.001_sd113_n2_n04_eps0.0_min0.0_max1.0_shift0.0\"\n",
    "path_2 = os.path.join(\"saved_models\", relative_path_3)\n",
    "path_3 = os.path.join(\"saved_models\", relative_path_3)\n",
    "model_2 = CirclePF(\n",
    "    hidden_dim=hidden_dim,\n",
    "    n_hidden=n_hidden,\n",
    "    n_components=n_components,\n",
    "    n_components_s0=n_components_s0,\n",
    "    beta_min=beta_min,\n",
    "    beta_max=beta_max,\n",
    ")\n",
    "model_2.load_state_dict(torch.load(os.path.join(path_2, \"model.pt\")))\n",
    "model_3 = CirclePF(\n",
    "    hidden_dim=hidden_dim,\n",
    "    n_hidden=n_hidden,\n",
    "    n_components=n_components,\n",
    "    n_components_s0=n_components_s0,\n",
    "    beta_min=beta_min,\n",
    "    beta_max=beta_max,\n",
    ")\n",
    "model_3.load_state_dict(torch.load(os.path.join(path_3, \"model.pt\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_actions(\n",
    "    model, states, min_terminate_proba=0.0, max_terminate_proba=1.0, epsilon=0.0\n",
    "):\n",
    "    # with probability epsilon, sample uniformly in the quarter circle\n",
    "    # states is a tensor of shape (n, dim)\n",
    "    batch_size = states.shape[0]\n",
    "    out = model.to_dist(states)\n",
    "    if isinstance(out[0], Distribution):  # s0 input\n",
    "        dist_r, dist_theta = out\n",
    "        samples_r = dist_r.sample(torch.Size((batch_size,)))\n",
    "        samples_theta = dist_theta.sample(torch.Size((batch_size,)))\n",
    "        if epsilon > 0:\n",
    "            uniform_mask = torch.rand(batch_size) < epsilon\n",
    "            samples_r[uniform_mask] = torch.rand_like(samples_r[uniform_mask])\n",
    "            samples_theta[uniform_mask] = torch.rand_like(samples_theta[uniform_mask])\n",
    "        actions = (\n",
    "            torch.stack(\n",
    "                [\n",
    "                    samples_r * torch.cos(torch.pi / 2.0 * samples_theta),\n",
    "                    samples_r * torch.sin(torch.pi / 2.0 * samples_theta),\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "            * env.delta\n",
    "        )\n",
    "        logprobs = (\n",
    "            dist_r.log_prob(samples_r)\n",
    "            + dist_theta.log_prob(samples_theta)\n",
    "            # + np.log(4 / (env.delta**2 * np.pi))  # debugging\n",
    "            - torch.log(samples_r * env.delta)\n",
    "            - np.log(np.pi / 2)\n",
    "            - np.log(env.delta)  # why ?\n",
    "        )\n",
    "        # print(\"logprobs\", logprobs.exp().mean(), logprobs.exp().std())\n",
    "        # print(\n",
    "        #     (2 * dist_r.log_prob(samples_r).exp() / samples_r).mean(),  # pdf of radius for uniform in disk\n",
    "        #     (2 * dist_r.log_prob(samples_r).exp() / samples_r).std(),\n",
    "        # )\n",
    "        # print(\n",
    "        #     dist_theta.log_prob(samples_theta).exp().mean(),\n",
    "        #     dist_theta.log_prob(samples_theta).exp().std(),\n",
    "        # )\n",
    "        if n_components == 1 and n_components_s0 == 1:\n",
    "            entropies = dist_r.entropy() + dist_theta.entropy()\n",
    "        else:\n",
    "            entropies = torch.zeros(\n",
    "                batch_size\n",
    "            )  # the previous line is not implemented in general\n",
    "    else:\n",
    "        exit_proba, dist = out\n",
    "        exit_proba = exit_proba.clamp(min_terminate_proba, max_terminate_proba)\n",
    "\n",
    "        exit = torch.bernoulli(exit_proba).bool()\n",
    "        exit[torch.norm(1 - states, dim=1) <= env.delta] = True\n",
    "        exit[torch.any(states >= 1 - env.epsilon, dim=-1)] = True\n",
    "        A = torch.where(\n",
    "            states[:, 0] <= 1 - env.delta,\n",
    "            0.0,\n",
    "            2.0 / torch.pi * torch.arccos((1 - states[:, 0]) / env.delta),\n",
    "        )\n",
    "        B = torch.where(\n",
    "            states[:, 1] <= 1 - env.delta,\n",
    "            1.0,\n",
    "            2.0 / torch.pi * torch.arcsin((1 - states[:, 1]) / env.delta),\n",
    "        )\n",
    "        assert torch.all(\n",
    "            B[~torch.any(states >= 1 - env.delta, dim=-1)]\n",
    "            >= A[~torch.any(states >= 1 - env.delta, dim=-1)]\n",
    "        )\n",
    "        samples = dist.sample()\n",
    "        if epsilon > 0:\n",
    "            uniform_mask = torch.rand(batch_size) < epsilon\n",
    "            samples[uniform_mask] = torch.rand_like(samples[uniform_mask])\n",
    "        actions = samples * (B - A) + A\n",
    "        actions *= torch.pi / 2.0\n",
    "        actions = (\n",
    "            torch.stack([torch.cos(actions), torch.sin(actions)], dim=1) * env.delta\n",
    "        )\n",
    "\n",
    "        logprobs = (\n",
    "            dist.log_prob(samples)\n",
    "            + torch.log(1 - exit_proba)\n",
    "            - np.log(env.delta)\n",
    "            - np.log(np.pi / 2)\n",
    "            - torch.log(B - A)\n",
    "            # + torch.log(2.0 / (torch.pi * env.delta * (B - A)))\n",
    "            # - 0.5 * torch.log(1 - torch.cos(actions[:, 0]) ** 2)\n",
    "        )\n",
    "\n",
    "        actions[exit] = -float(\"inf\")\n",
    "        logprobs[exit] = torch.log(exit_proba[exit])\n",
    "        logprobs[torch.norm(1 - states, dim=1) <= env.delta] = 0.0\n",
    "        logprobs[torch.any(states >= 1 - env.epsilon, dim=-1)] = 0.0\n",
    "\n",
    "        if n_components == 1 and n_components_s0 == 1:\n",
    "            entropies = (\n",
    "                -exit_proba * torch.log(exit_proba)\n",
    "                - (1 - exit_proba) * torch.log(1 - exit_proba)\n",
    "                + dist.entropy() * (1 - exit_proba)\n",
    "            )\n",
    "        else:\n",
    "            entropies = torch.zeros(\n",
    "                batch_size\n",
    "            )  # the previous line is not implemented in general\n",
    "\n",
    "    return actions, logprobs, entropies\n",
    "\n",
    "\n",
    "def sample_trajectories(\n",
    "    model, n_trajectories, min_terminate_proba=0.0, max_terminate_proba=1.0, epsilon=0.0\n",
    "):\n",
    "    states = torch.zeros((n_trajectories, env.dim), device=env.device)\n",
    "    actionss = []\n",
    "    trajectories = [states]\n",
    "    trajectories_logprobs = torch.zeros((n_trajectories,), device=env.device)\n",
    "    trajectories_entropies = torch.zeros((n_trajectories,), device=env.device)\n",
    "    while not torch.all(states == env.sink_state):\n",
    "        non_terminal_mask = torch.all(states != env.sink_state, dim=-1)\n",
    "        actions = torch.full(\n",
    "            (n_trajectories, env.dim), -float(\"inf\"), device=env.device\n",
    "        )\n",
    "        non_terminal_actions, logprobs, entropies = sample_actions(\n",
    "            model,\n",
    "            states[non_terminal_mask],\n",
    "            min_terminate_proba=min_terminate_proba,\n",
    "            max_terminate_proba=max_terminate_proba,\n",
    "            epsilon=epsilon,\n",
    "        )\n",
    "        actions[non_terminal_mask] = non_terminal_actions.reshape(-1, env.dim)\n",
    "        actionss.append(actions)\n",
    "        states = env.step(states, actions)\n",
    "        trajectories.append(states)\n",
    "        trajectories_logprobs[non_terminal_mask] += logprobs\n",
    "        trajectories_entropies[non_terminal_mask] += entropies\n",
    "    trajectories = torch.stack(trajectories, dim=1)\n",
    "    actionss = torch.stack(actionss, dim=1)\n",
    "    return trajectories, actionss, trajectories_logprobs, trajectories_entropies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories, _, _, _ = sample_trajectories(model, 20000)\n",
    "last_states_PF = get_last_states(env, trajectories)\n",
    "\n",
    "trajectories_2, _, _, _ = sample_trajectories(model_2, 20000)\n",
    "last_states_PF_2 = get_last_states(env, trajectories)\n",
    "\n",
    "trajectories_3, _, _, _ = sample_trajectories(model_3, 20000)\n",
    "last_states_PF_3 = get_last_states(env, trajectories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde_pf2 = KernelDensity(kernel=\"epanechnikov\",bandwidth=0.1).fit(\n",
    "    # last_states_PF.numpy()\n",
    "    torch.cat([last_states_PF, last_states_PF_2, last_states_PF_3], dim=0).numpy()\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_states, n_2 = get_test_states(n=200)\n",
    "log_dens_PF2 = kde_pf2.score_samples(test_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0_batch = torch.zeros((20000, env.dim), device=env.device)\n",
    "actions_s0 = sample_actions(model, s0_batch)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde_s0 = KernelDensity(kernel=\"gaussian\",\n",
    "     bandwidth=0.01).fit(actions_s0.numpy())\n",
    "test_states, n_2 = get_test_states(maxi=env.delta, n=200)\n",
    "log_dens_s0 = kde_s0.score_samples(test_states)\n",
    "log_dens_s0[np.linalg.norm(test_states, axis=1) > env.delta] = np.nan\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sample_from_reward(env, 20000)\n",
    "current_states_bw = torch.clone(samples)\n",
    "while True:\n",
    "    A = torch.where(\n",
    "            current_states_bw[:, 0] >= env.delta,\n",
    "            0.0,\n",
    "            2.0 / torch.pi * torch.arccos((current_states_bw[:, 0]) / env.delta),\n",
    "        )\n",
    "    B = torch.where(\n",
    "        current_states_bw[:, 1] >= env.delta,\n",
    "        1.0,\n",
    "        2.0 / torch.pi * torch.arcsin((current_states_bw[:, 1]) / env.delta),\n",
    "    )\n",
    "    dist = bw_model.to_dist(current_states_bw)\n",
    "    theta = dist.sample()\n",
    "\n",
    "    theta = theta * (B - A) + A\n",
    "    to_change_mask = current_states_bw.norm(dim=1) > env.delta\n",
    "    if to_change_mask.sum() == 0:\n",
    "        break\n",
    "    current_states_bw[to_change_mask] =current_states_bw[to_change_mask] - env.delta * torch.stack([torch.cos(np.pi / 2 * theta[to_change_mask]), torch.sin(np.pi / 2 * theta[to_change_mask])], dim=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde_bw = KernelDensity(kernel=\"gaussian\",\n",
    "     bandwidth=0.01).fit(current_states_bw.numpy())\n",
    "test_states, n_2 = get_test_states(maxi=env.delta, n=200)\n",
    "log_dens_bw = kde_bw.score_samples(test_states)\n",
    "log_dens_bw[np.linalg.norm(test_states, axis=1) > env.delta] = np.nan\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ONE PLOT !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the reward and the previous kde on the same, reward on the left, kde on the right\n",
    "for cmap in [\"viridis\", \"turbo\", \"plasma\", \"gnuplot\"]:\n",
    "\n",
    "    fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1, 5, figsize=(16, 10))\n",
    "    test_states, n_2 = get_test_states(n=200)\n",
    "    reward = env.reward(torch.FloatTensor(test_states))\n",
    "    reward = reward.reshape(n_2, n_2)\n",
    "    ax1.imshow(reward, origin=\"lower\", extent=[0, 1, 0, 1], cmap=cmap, vmin=0.)\n",
    "    ax1.set_xticks([0, 1])\n",
    "    ax1.set_xticklabels(['0', '1'])\n",
    "    ax1.tick_params(axis=u'both', which=u'both',length=0)\n",
    "    ax1.set_yticks([])\n",
    "    ax2.imshow((log_dens_PF2).reshape(n_2, n_2), origin=\"lower\", extent=[0, 1., 0, 1.], cmap=cmap, vmin=-0.1, vmax=1.3)\n",
    "    ax2.set_xticks([0, 1])\n",
    "    ax2.set_xticklabels(['0', '1'])\n",
    "    ax2.tick_params(axis=u'both', which=u'both',length=0)\n",
    "    ax2.set_yticks([])\n",
    "    ax3.imshow((log_dens_uniform_pb).reshape(n_2, n_2), origin=\"lower\", extent=[0, 0.25, 0, 0.25], cmap=cmap, vmin=2, vmax=3.5)\n",
    "    # leave only the leftmost and rightmost ticks - just the text, no ticks\n",
    "    ax3.set_xticks([0, 0.25])\n",
    "    ax3.set_xticklabels(['0', r'$\\rho$'])\n",
    "    ax3.tick_params(axis=u'both', which=u'both',length=0)\n",
    "    ax3.set_yticks([])\n",
    "    ax4.imshow((log_dens_s0).reshape(n_2, n_2), origin=\"lower\", extent=[0, 0.25, 0, 0.25], cmap=cmap, vmin=2, vmax=3.5)\n",
    "    ax4.set_xticks([0, 0.25])\n",
    "    ax4.set_xticklabels(['0', r'$\\rho$'])\n",
    "    ax4.tick_params(axis=u'both', which=u'both',length=0)\n",
    "    ax4.set_yticks([])\n",
    "\n",
    "    ax5.imshow((log_dens_bw).reshape(n_2, n_2), origin=\"lower\", extent=[0, 0.25, 0, 0.25], cmap=cmap, vmin=2, vmax=3.5)\n",
    "    ax5.set_xticks([0, 0.25])\n",
    "    ax5.set_xticklabels(['0', r'$\\rho$'])\n",
    "    ax5.tick_params(axis=u'both', which=u'both',length=0)\n",
    "    ax5.set_yticks([])\n",
    "    # remove boundaries\n",
    "    for ax in [ax1, ax2, ax3, ax4, ax5]:\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(False)\n",
    "\n",
    "    # Change the fontsize  and font of the ticks\n",
    "    for ax in [ax1, ax2, ax3, ax4, ax5]:\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        ax.tick_params(axis='both', which='minor', labelsize=20)\n",
    "        ax.xaxis.label.set_fontsize(20)\n",
    "\n",
    "    if cmap == \"viridis\":\n",
    "        plt.savefig(\"reward_and_kde.pdf\", bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gfn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9c29a775735f82fff5eeeac160a0a52825744fde233859deac6552b73fecc3ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
